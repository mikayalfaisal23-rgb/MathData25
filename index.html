<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>MathData 2025 ‚Äì Day vs Night Classification</title>

<style>
    body {
        margin: 0;
        font-family: "Inter", Arial, sans-serif;
        background: #faf9f7;
        color: #333;
        line-height: 1.6;
    }

    header {
        background: linear-gradient(135deg, #4b79a1, #283e51);
        padding: 60px 20px;
        text-align: center;
        color: #fff;
    }

    header h1 {
        margin: 0;
        font-size: 2.8rem;
        letter-spacing: 1px;
    }

    header p {
        opacity: 0.9;
        font-size: 1.2rem;
    }

    .container {
        width: 90%;
        max-width: 900px;
        margin: auto;
        padding: 20px;
    }

    .section {
        background: #fff;
        padding: 25px;
        margin: 20px 0;
        border-radius: 14px;
        box-shadow: 0 4px 12px rgba(0,0,0,0.08);
        transition: transform 0.15s ease;
    }

    .section:hover {
        transform: translateY(-3px);
    }

    h2 {
        color: #283e51;
        margin-top: 0;
        font-size: 1.8rem;
    }

    h3 {
        color: #4b79a1;
        margin-top: 15px;
        font-size: 1.4rem;
    }

    ul {
        margin-top: 10px;
    }

    .code {
        background: #f1f1f4;
        padding: 12px;
        border-radius: 8px;
        font-family: monospace;
        overflow-x: auto;
    }

    img.chart {
        width: 100%;
        margin: 15px 0;
        border-radius: 12px;
        box-shadow: 0 3px 10px rgba(0,0,0,0.1);
    }

    footer {
        background: #283e51;
        text-align: center;
        color: #fff;
        padding: 25px;
        margin-top: 40px;
    }
</style>
</head>

<body>

<header>
    <h1>MathData 2025</h1>
    <p>Day vs Night Image Classification Using Deep Learning</p>
</header>

<div class="container">

    <div class="section">
        <h2>üåûüåô Project Overview</h2>
        <p>
            This project explores the task of distinguishing <strong>daytime</strong> from 
            <strong>nighttime</strong> images using convolutional neural networks. 
            I trained multiple architectures, compared hyperparameters, and analyzed the 
            effects of augmentation ‚Äî all fully logged using <strong>Weights & Biases</strong>.
        </p>
    </div>

    <!-- NEW SECTION: Problem and Motivation -->
    <div class="section">
        <h2>üí° Problem and Motivation</h2>
        <p>
            With rising populations, climate change, as well as quickly falling ecosystems and 
            health levels, the environment is arguably the most important thing we need to take 
            care of. Because how can we develop and prosper science, technology and life, if we 
            cannot even live on this planet healthily? Fighting pollution and waste is key, and 
            even though it doesn‚Äôt solve all our problems, starting off with light and energy 
            pollution is very important.
        </p>
        <p>
            By using day and night images, we can reduce light usage, lower costs, develop 
            surveillance technology, and even use it to monitor beaches and coastlines for massive 
            amounts of information (best time to go, sea pollution, dangerous animals, etc.).
        </p>
        <p>
            Your health is the most important thing you have, because without it you cannot do anything.
        </p>
    </div>

    <!-- NEW SECTION: Technical Approach With AlexNet -->
    <div class="section">
        <h2>üõ†Ô∏è Technical Approach With AlexNet</h2>
        <p>
            First I gathered and downloaded hundreds of images manually from the web as well as 
            different websites. This was followed by making a GitHub repository and uploading 
            these files into different test and train folders. Then I used AlexNet and PyTorch 
            to separate these images and run countless tests, coding all of it in Google 
            Collaboratory with Python.
        </p>
        <p>
            After going back and forth and fixing all the errors, I managed to produce multiple 
            different graphs and visuals. Finally, I made a website (linked to GitHub) explaining 
            my entire problem and solution, as well as a video, presentation, and scientific poster.
        </p>
    </div>

    <div class="section">
        <h2>üìÅ Dataset</h2>
        <p>The dataset contains two folders:</p>
        <ul>
            <li><strong>day/</strong> ‚Äì images captured in daylight</li>
            <li><strong>night/</strong> ‚Äì images captured in low-light/nighttime</li>
        </ul>
        <p>
            All images were preprocessed to 227√ó227 pixels for AlexNet compatibility.
        </p>
    </div>

    <div class="section">
        <h2>üß† Models Trained</h2>
        <p>I experimented with three architectures:</p>
        <ul>
            <li><strong>AlexNet</strong> (custom implementation)</li>
            <li><strong>VGG-16</strong> (pretrained)</li>
            <li><strong>ResNet-18</strong> (pretrained)</li>
        </ul>
        <p>
            Each model was trained using PyTorch and evaluated on the same test set for fairness.
        </p>
    </div>

    <div class="section">
        <h2>‚öóÔ∏è Experiments</h2>
        <p>I ran a structured set of experiments required by the course:</p>

        <ul>
            <li>Batch Size: <strong>16 vs 64</strong></li>
            <li>Learning Rate: <strong>0.001 vs 0.0001</strong></li>
            <li>With vs Without Data Augmentation</li>
            <li>Architecture comparison (AlexNet vs VGG16 vs ResNet18)</li>
        </ul>

        <p>Each run was tracked with Weights & Biases.</p>

        <div class="code">
            {"batch_size": 32, "lr": 0.001, "augmentation": true, "model": "alexnet"}
        </div>
    </div>

    <!-- UPDATED RESULTS SECTION -->
    <div class="section">
        <h2>üìä Results</h2>
        <p>
            Below are the real results generated from the training experiments.  
            These charts are produced in Google Colab and saved directly into the GitHub repository.
        </p>

        <h3>üìà Accuracy Comparison (All Experiments)</h3>
        <p>This bar chart compares every experiment.</p>
        <img class="chart" src="accuracy_comparison.png" alt="Accuracy Comparison Bar Chart">

        <h3>üìâ Test Accuracy Curves</h3>
        <p>This figure shows accuracy across epochs.</p>
        <img class="chart" src="test_accuracy_curves.png" alt="Test Accuracy Curves">

        <h3>üìä Accuracy Over Epochs</h3>
        <p>This line plot shows training vs testing accuracy over epochs.</p>
        <img class="chart" src="accuracy_over_epochs.png" alt="Accuracy Over Epochs">
    </div>

    <div class="section">
        <h2>üìú About the Code</h2>
        <p>The full training pipeline is implemented in Python using PyTorch. Here is a simplified view:</p>

        <div class="code">
            def train_model(model, train_loader, test_loader):<br>
            &nbsp;&nbsp;&nbsp;&nbsp;for epoch in range(epochs):<br>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# training<br>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# evaluation<br>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# wandb logging<br>
        </div>
    </div>

</div>

<footer>
    <p>MathData 2025 ‚Ä¢ Built with ‚ù§Ô∏è using PyTorch & Weights & Biases</p>
</footer>

</body>
</html>
